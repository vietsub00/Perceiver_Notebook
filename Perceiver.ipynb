{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceiver_Self_Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vietsub00/Perceiver_Notebook/blob/main/Perceiver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY7ht2vQOTKi",
        "outputId": "ace2a77b-05cb-4806-92c2-2b373a0cc8bb"
      },
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install einops"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n",
            "Collecting einops\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z8io-gQOgy4"
      },
      "source": [
        "import numpy as np\n",
        "from math import pi\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "from einops import rearrange, repeat"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwmi0PZbgNol",
        "outputId": "13df2ec9-1763-4d7e-f64e-702b91d7ac3c"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.100.76.114:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.100.76.114:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRkCTh6yYMtb"
      },
      "source": [
        "## Setup Hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clCuATqhO4dS"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "num_classes = 10\n",
        "img_size = 32\n",
        "data_dim = img_size**2\n",
        "num_freq_bands = 6          # number of freq bands, with original value (2 * K + 1)\n",
        "max_freq = 10.0             # maximum frequency, hyperparameter depending on how fine the data is\n",
        "batch_size = 64\n",
        "num_epochs = 150     \n",
        "ffn_dropout_rate = 0.0\n",
        "attn_dropout_rate = 0.0\n",
        "latent_dim = 256      # Size of the latent array.\n",
        "projection_dim = 29   # Embedding size of each element in the data and latent arrays.\n",
        "num_heads = 6         # Number of Transformer heads.\n",
        "ffn_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "    ]                 # Size of the Transformer Feedforward network.\n",
        "num_transformer_blocks = 6\n",
        "num_iterations = 8    # Repetitions of the cross-attention and Transformer modules.\n",
        "classifier_units = [\n",
        "    projection_dim,\n",
        "    num_classes,\n",
        "]                      # Size of the Feedforward network of the final classifier."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnImlwcGPi7o"
      },
      "source": [
        "## Load and preprocess the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDY9XgJoOpSr",
        "outputId": "7ce49975-1908-4af5-e1e4-3c142aee988a"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "print(f'x_train shape: {x_train.shape}, y_train shape: {y_train.shape}')\n",
        "print(f'x_train shape: {x_test.shape}, y_train shape: {y_test.shape}')\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3), y_train shape: (50000, 1)\n",
            "x_train shape: (10000, 32, 32, 3), y_train shape: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fourier_encode(x, max_freq, num_bands = 4):\n",
        "  x = tf.expand_dims(x, -1)\n",
        "  orig_x = x\n",
        "\n",
        "  scales = tf.linspace(1., max_freq / 2, num_bands)\n",
        "  scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
        "\n",
        "  x = x * scales * pi\n",
        "  x = tf.concat([tf.sin(x), tf.cos(x)], axis = -1)\n",
        "  x = tf.concat((x, orig_x), axis = -1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "zqjGQheZz633"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_freq = 10.0\n",
        "num_freq_bands = 6\n",
        "\n",
        "b, *axis, _ = x_train.shape\n",
        "\n",
        "axis_pos = list(map(lambda size: tf.linspace(-1.0, 1.0, num = size), axis))\n",
        "pos = tf.stack(tf.meshgrid(*axis_pos, indexing = \"ij\"), axis = -1)\n",
        "enc_pos = fourier_encode(pos, max_freq, num_freq_bands)\n",
        "enc_pos = rearrange(enc_pos, \"... n d -> ... (n d)\")\n",
        "enc_pos = repeat(enc_pos, \"... -> b ...\", b = b)\n",
        "\n",
        "x_train = tf.concat((x_train, enc_pos), axis = -1)\n",
        "x_train = rearrange(x_train, \"b ... d -> b (...) d\")\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "id": "OZwFfzs-z-9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdDUF_Ebout2"
      },
      "source": [
        "## Helper Functions:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def geglu(x):\n",
        "  x, gates = tf.split(x, 2, axis=-1)\n",
        "  return x * tf.nn.gelu(gates)"
      ],
      "metadata": {
        "id": "XzCPOzuF0fwv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ffn(hidden_units, dropout_rate=0.0, activation='geglu'):\n",
        "    ffn_layers = []\n",
        "    for units in hidden_units[:-1]:\n",
        "        ffn_layers.append(layers.Dense(units, activation='relu'))\n",
        "\n",
        "    ffn_layers.append(layers.Dense(units=hidden_units[-1]))\n",
        "    ffn_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "    ffn = tf.keras.Sequential(ffn_layers)\n",
        "    return ffn"
      ],
      "metadata": {
        "id": "rp2m7Ik66UvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_attention_module(latent_dim, \n",
        "                           data_dim, \n",
        "                           projection_dim, \n",
        "                           ffn_units, \n",
        "                           ffn_dropout_rate = 0,\n",
        "                           attn_dropout_rate = 0\n",
        "):\n",
        "    ''' According to the paper, the latent array has shape (N, D), which \n",
        "    corresponding to shape (latent_dim, projection_dim) in this function'''\n",
        "\n",
        "    inputs = {\n",
        "        # Recieve the latent array as an input of shape [1, latent_dim, projection_dim].\n",
        "        \"latent_array\": layers.Input(shape=(latent_dim, projection_dim)),\n",
        "        # Recieve the data_array (encoded image) as an input of shape [batch_size, data_dim, projection_dim].\n",
        "        \"data_array\": layers.Input(shape=(data_dim, projection_dim))\n",
        "    }\n",
        "\n",
        "    # Apply layer norm to the inputs\n",
        "    latent_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"latent_array\"])\n",
        "    data_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"data_array\"])\n",
        "\n",
        "    # Create query tensor: [1, latent_dim, projection_dim].\n",
        "    query = layers.Dense(units=projection_dim)(latent_array)\n",
        "    # Create key tensor: [batch_size, data_dim, projection_dim].\n",
        "    key = layers.Dense(units=projection_dim)(data_array)\n",
        "    # Create value tensor: [batch_size, data_dim, projection_dim].\n",
        "    value = layers.Dense(units=projection_dim)(data_array)\n",
        "\n",
        "    # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n",
        "    attention_output = layers.Attention(use_scale=True, dropout=attn_dropout_rate)(\n",
        "        [query, key, value], return_attention_scores=False\n",
        "    )\n",
        "    # Skip connection 1.\n",
        "    attention_output = layers.Add()([attention_output, latent_array])\n",
        "\n",
        "    # Apply layer norm.\n",
        "    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "    # Apply Feedforward network.\n",
        "    ffn = create_ffn(hidden_units=ffn_units, dropout_rate=ffn_dropout_rate)\n",
        "    outputs = ffn(attention_output)\n",
        "    # Skip connection 2.\n",
        "    outputs = layers.Add()([outputs, attention_output])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "qoU3jmu1_HCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def latent_transformer_module(latent_dim,\n",
        "                              projection_dim,\n",
        "                              num_heads,\n",
        "                              num_transformer_blocks,\n",
        "                              ffn_units,\n",
        "                              ffn_dropout_rate = 0,\n",
        "                              attn_dropout_rate = 0\n",
        "):\n",
        "\n",
        "    # input_shape: [1, latent_dim, projection_dim]\n",
        "    inputs = layers.Input(shape=(latent_dim, projection_dim))\n",
        "\n",
        "    x0 = inputs\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        # Apply layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x0)\n",
        "        # Create a multi-head self-attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x0])\n",
        "        # Apply layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # Apply Feedforward network.\n",
        "        ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "        x3 = ffn(x3)\n",
        "        # Skip connection 2.\n",
        "        x0 = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=x0)\n",
        "    return model"
      ],
      "metadata": {
        "id": "8bl6Nwt0AE6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model:"
      ],
      "metadata": {
        "id": "YA1129BB0Zp6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdmELRikPrJU"
      },
      "source": [
        "def create_model():\n",
        "  input = layers.Input(input_shape)\n",
        "  # x = layers.Dropout(0.3)(x)\n",
        "  #x = layers.Dense(512, activation='relu')(x)\n",
        "  # x = layers.Flatten()(x)\n",
        "  #x = layers.Dense(2048, activation='relu')(x)\n",
        "  # x = layers.Dense(1024, activation='relu')(x)\n",
        "  # x = layers.Dropout(0.5)(x)\n",
        "  # x = layers.Dense(512, activation='relu')(x)\n",
        "  # x = layers.Dropout(0.5)(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(256, activation='relu')(x)\n",
        "  x = layers.Dense(64, activation='relu')(x)\n",
        "  output = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "  model = keras.Model(input, output)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD0uhabcT6Pj"
      },
      "source": [
        "with strategy.scope():\n",
        "  model = create_model()\n",
        "  model.compile(optimizer='Adam', \n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=[keras.metrics.SparseCategoricalAccuracy(name='acc')])\n",
        "  model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXVezPvJMUXO"
      },
      "source": [
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=4, factor=0.5)\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCTwRYYDQIvf"
      },
      "source": [
        "with strategy.scope():\n",
        "  history = model.fit(x=data,\n",
        "                      y=y_train,\n",
        "                      batch_size=64,\n",
        "                      epochs=100, \n",
        "                      callbacks=[reduce_lr, early_stopping],\n",
        "                      validation_split=0.2\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhFTGJitA53L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}